{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58abc494",
   "metadata": {},
   "source": [
    "# RDD\n",
    "\n",
    "* Immutable: operations on it generate another RDD\n",
    "* Transformation x Action: Lazy x in-time computation\n",
    "* RDD = Low Level. Dataset and DataFrame: High Level\n",
    "* Dataset: Java/Scala. Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d562eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/08 08:13:48 WARN Utils: Your hostname, WIN-NJTBBD8GS0T, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/08 08:13:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 08:13:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/08 08:13:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Cria (ou recupera) uma SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExemploRDD\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4576d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Usando o SparkContext para paralelizar uma lista\n",
    "\n",
    "# O método parallelize é uma forma de criar um RDD a partir de uma coleção local (por exemplo, uma lista em Python).\n",
    "# Ele faz duas coisas principais:\n",
    "# Divide (paraleliza) os dados locais em partições.\n",
    "# Distribui essas partições entre os nós do cluster Spark.\n",
    "rdd = spark.sparkContext.parallelize([i for i in range(11)])\n",
    "\n",
    "print(rdd.take(3))\n",
    "print(rdd.top(3))\n",
    "print(rdd.collect())\n",
    "\n",
    "# max, min, count, mean, stdev, ... same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6256dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476f9176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 6, 7, 10]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "rdd.sample(False, 0.5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdc826d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map\n",
    "\n",
    "rdd.map(lambda x: x ** 2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be3ca76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (0, 6), (0, 7), (0, 8), (0, 9), (0, 10), (0, 11), (0, 12), (0, 13), (0, 14), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (1, 14), (2, 5), (2, 6), (2, 7), (2, 8), (2, 9), (2, 10), (2, 11), (2, 12), (2, 13), (2, 14), (3, 5), (3, 6), (3, 7), (3, 8), (3, 9), (3, 10), (3, 11), (3, 12), (3, 13), (3, 14), (4, 5), (4, 6), (4, 7), (4, 8), (4, 9), (4, 10), (4, 11), (4, 12), (4, 13), (4, 14), (5, 5), (5, 6), (5, 7), (5, 8), (5, 9), (5, 10), (5, 11), (5, 12), (5, 13), (5, 14), (6, 5), (6, 6), (6, 7), (6, 8), (6, 9), (6, 10), (6, 11), (6, 12), (6, 13), (6, 14), (7, 5), (7, 6), (7, 7), (7, 8), (7, 9), (7, 10), (7, 11), (7, 12), (7, 13), (7, 14), (8, 5), (8, 6), (8, 7), (8, 8), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (8, 14), (9, 5), (9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (9, 11), (9, 12), (9, 13), (9, 14), (10, 5), (10, 6), (10, 7), (10, 8), (10, 9), (10, 10), (10, 11), (10, 12), (10, 13), (10, 14)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>(143 + 1) / 144]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {(0, 5): 1, (0, 6): 1, (0, 7): 1, (0, 8): 1, (0, 9): 1, (0, 10): 1, (0, 11): 1, (0, 12): 1, (0, 13): 1, (0, 14): 1, (1, 5): 1, (1, 6): 1, (1, 7): 1, (1, 8): 1, (1, 9): 1, (1, 10): 1, (1, 11): 1, (1, 12): 1, (1, 13): 1, (1, 14): 1, (2, 5): 1, (2, 6): 1, (2, 7): 1, (2, 8): 1, (2, 9): 1, (2, 10): 1, (2, 11): 1, (2, 12): 1, (2, 13): 1, (2, 14): 1, (3, 5): 1, (3, 6): 1, (3, 7): 1, (3, 8): 1, (3, 9): 1, (3, 10): 1, (3, 11): 1, (3, 12): 1, (3, 13): 1, (3, 14): 1, (4, 5): 1, (4, 6): 1, (4, 7): 1, (4, 8): 1, (4, 9): 1, (4, 10): 1, (4, 11): 1, (4, 12): 1, (4, 13): 1, (4, 14): 1, (5, 5): 1, (5, 6): 1, (5, 7): 1, (5, 8): 1, (5, 9): 1, (5, 10): 1, (5, 11): 1, (5, 12): 1, (5, 13): 1, (5, 14): 1, (6, 5): 1, (6, 6): 1, (6, 7): 1, (6, 8): 1, (6, 9): 1, (6, 10): 1, (6, 11): 1, (6, 12): 1, (6, 13): 1, (6, 14): 1, (7, 5): 1, (7, 6): 1, (7, 7): 1, (7, 8): 1, (7, 9): 1, (7, 10): 1, (7, 11): 1, (7, 12): 1, (7, 13): 1, (7, 14): 1, (8, 5): 1, (8, 6): 1, (8, 7): 1, (8, 8): 1, (8, 9): 1, (8, 10): 1, (8, 11): 1, (8, 12): 1, (8, 13): 1, (8, 14): 1, (9, 5): 1, (9, 6): 1, (9, 7): 1, (9, 8): 1, (9, 9): 1, (9, 10): 1, (9, 11): 1, (9, 12): 1, (9, 13): 1, (9, 14): 1, (10, 5): 1, (10, 6): 1, (10, 7): 1, (10, 8): 1, (10, 9): 1, (10, 10): 1, (10, 11): 1, (10, 12): 1, (10, 13): 1, (10, 14): 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nums1 = spark.sparkContext.parallelize([i for i in range(11)])\n",
    "nums2 = spark.sparkContext.parallelize([i for i in range(5, 15)])\n",
    "\n",
    "# No duplicates removal!\n",
    "print(nums1.union(nums2).collect())\n",
    "\n",
    "# Slow! Deal with comparing data from different slices\n",
    "print(nums1.intersection(nums2).collect())\n",
    "\n",
    "# Subtraction\n",
    "print(nums1.subtract(nums2).collect())\n",
    "\n",
    "# Cartesian product\n",
    "cartesian = nums1.cartesian(nums2)\n",
    "print(cartesian.collect())\n",
    "print(cartesian.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed33d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 120), (4, 250), (5, 78)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keys and values\n",
    "purchases = spark.sparkContext.parallelize([(1, 100), (2, 300), (3, 120), (4, 250), (5, 78)])\n",
    "debts = spark.sparkContext.parallelize([(1, 20), (2, 300)])\n",
    "# Available commands\n",
    "# purchases.keys(), purchases.values()\n",
    "# purchases.countByKey(), purchases.countByValues()\n",
    "# sum_1_purchases = purchases.mapValues(lambda s: s + 1)\n",
    "# purchases.join(debts).collect()\n",
    "# purchases.subtractByKey(debts).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".obenkyo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
