{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee966a2",
   "metadata": {},
   "source": [
    "# Study Guide\n",
    "\n",
    "* Quickstart: https://spark.apache.org/docs/latest/quick-start.html\n",
    "    * spark shell\n",
    "    * caching\n",
    "    * Spark Session: ``SparkSession.builder.appName(\"SimpleApp\").getOrCreate()``\n",
    "* https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
    "    * Abstractions:\n",
    "        * RDD\n",
    "        * shared variables: (_broadcast_, _accumulators_)\n",
    "        * DataFrame vs Datasets (Python vs strong typing)\n",
    "* Pyspark docs\n",
    "    * https://spark.apache.org/docs/latest/api/python/index.html\n",
    "        * pyspark-connect\n",
    "* Pyspark Pandas API (`pyspark.pandas.DataFrame`)\n",
    "    * https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_ps.html\n",
    "* Testing\n",
    "    * https://spark.apache.org/docs/latest/api/python/getting_started/testing_pyspark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acce7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "# sudo apt update\n",
    "# sudo apt install openjdk-17-jdk\n",
    "# !pip install pyspark\n",
    "# export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))\n",
    "# !pip install pyspark-connect\n",
    "# !pip install pyspark-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193eb79",
   "metadata": {},
   "source": [
    "# First Contact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1abc2a",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860c56a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/01 08:02:46 WARN Utils: Your hostname, WIN-NJTBBD8GS0T, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/09/01 08:02:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/01 08:02:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368c827",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c64f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataframe from pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1], \"b\": [2]})\n",
    "\n",
    "# We can do a lot of things from session object\n",
    "dfspark = spark.createDataFrame(df)\n",
    "dfspark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e777d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  3|\n",
      "|  1|  2|  3|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df_rows = spark.createDataFrame([\n",
    "    Row(a=1, b=2, c=3),\n",
    "    Row(a=1, b=2, c=3),\n",
    "    Row(a=1, b=2, c=3)],\n",
    "    schema='a string, b string, c int'\n",
    ")\n",
    "df_rows.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e79b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--\n",
      " a   | 1   \n",
      " b   | 2   \n",
      " c   | 3   \n",
      "-RECORD 1--\n",
      " a   | 1   \n",
      " b   | 2   \n",
      " c   | 3   \n",
      "-RECORD 2--\n",
      " a   | 1   \n",
      " b   | 2   \n",
      " c   | 3   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rows.show(3, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4732a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th></tr>\n",
       "<tr><td>1</td><td>2</td><td>3</td></tr>\n",
       "<tr><td>1</td><td>2</td><td>3</td></tr>\n",
       "<tr><td>1</td><td>2</td><td>3</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+---+\n",
       "|  a|  b|  c|\n",
       "+---+---+---+\n",
       "|  1|  2|  3|\n",
       "|  1|  2|  3|\n",
       "|  1|  2|  3|\n",
       "+---+---+---+"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying options\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "# spark.sql.repl.eagerEval.maxNumRows for max number of rows to show\n",
    "# spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", False)\n",
    "df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52576a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: string (nullable = true)\n",
      " |-- c: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df_rows.printSchema()), display(df_rows.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4ef8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+---+\n",
      "|summary|  a|  b|  c|\n",
      "+-------+---+---+---+\n",
      "|  count|  3|  3|  3|\n",
      "|   mean|1.0|2.0|3.0|\n",
      "| stddev|0.0|0.0|0.0|\n",
      "|    min|  1|  2|  3|\n",
      "|    max|  1|  2|  3|\n",
      "+-------+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rows.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5c8592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a='1', b='2', c=3), Row(a='1', b='2', c=3), Row(a='1', b='2', c=3)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect distributed data to local. Risk of out-of-memory error. Not lazy operation\n",
    "df_rows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0277c5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(a='1', b='2', c=3), Row(a='1', b='2', c=3)], [Row(a='1', b='2', c=3)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame.head(), tail() equivalent\n",
    "df_rows.take(2), df_rows.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8ddc139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  c\n",
       "0  1  2  3\n",
       "1  1  2  3\n",
       "2  1  2  3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rows.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390838b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lazy!\n",
    "df_rows.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e09dd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: string (nullable = true)\n",
      " |-- c: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rows.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aacd952",
   "metadata": {},
   "source": [
    "### Pandas + Spark: The power of UDFs (User Defined Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a76a9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=================================>                        (4 + 3) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(c)|\n",
      "+------------------+\n",
      "|                 4|\n",
      "|                 4|\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# \"apply\" example: user defined function (UDF)\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df_rows.select(pandas_plus_one(df_rows.c)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a68d9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = spark.createDataFrame(\n",
    "    [\n",
    "        Row(a=1, b=2, c=3),\n",
    "        Row(a=1, b=2, c=3),\n",
    "        Row(a=1, b=2, c=3),\n",
    "        Row(a=2, b=20, c=30),\n",
    "        Row(a=2, b=20, c=30),\n",
    "        Row(a=2, b=20, c=30),\n",
    "        Row(a=3, b=200, c=300),\n",
    "        Row(a=3, b=200, c=300),\n",
    "        Row(a=3, b=200, c=300),\n",
    "    ],\n",
    "    schema='a int, b int, c int'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6523c1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n",
      "|  a|avg(a)|avg(b)|avg(c)|\n",
      "+---+------+------+------+\n",
      "|  1|   1.0|   2.0|   3.0|\n",
      "|  2|   2.0|  20.0|  30.0|\n",
      "|  3|   3.0| 200.0| 300.0|\n",
      "+---+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_groupby.groupby(\"a\").avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ff9111d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('a', IntegerType(), True), StructField('b', IntegerType(), True), StructField('c', IntegerType(), True)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_groupby.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6b40f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+------+\n",
      "|  a|    b|    c|    v1|\n",
      "+---+-----+-----+------+\n",
      "|1.0|  2.0|  3.0|  -1.0|\n",
      "|1.0|  2.0|  3.0|  -1.0|\n",
      "|1.0|  2.0|  3.0|  -1.0|\n",
      "|2.0| 20.0| 30.0| -10.0|\n",
      "|2.0| 20.0| 30.0| -10.0|\n",
      "|2.0| 20.0| 30.0| -10.0|\n",
      "|3.0|200.0|300.0|-100.0|\n",
      "|3.0|200.0|300.0|-100.0|\n",
      "|3.0|200.0|300.0|-100.0|\n",
      "+---+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.b - pandas_df.c.mean())\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"a\", DoubleType()),\n",
    "    StructField(\"b\", DoubleType()),\n",
    "    StructField(\"c\", DoubleType()),\n",
    "    StructField(\"v1\", DoubleType()),\n",
    "])\n",
    "\n",
    "df_groupby.groupby('a').applyInPandas(plus_mean, schema=schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36e049",
   "metadata": {},
   "source": [
    "## Spark SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b216e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As SQL\n",
    "\n",
    "df_groupby.createOrReplaceTempView(\"table_groupby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3061ddf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>count(1)</th></tr>\n",
       "<tr><td>9</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|count(1)|\n",
       "+--------+\n",
       "|       9|\n",
       "+--------+"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(1) FROM table_groupby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8119cb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/01 08:16:44 WARN SimpleFunctionRegistry: The function add_one replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|add_one(a)|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         4|\n",
      "|         4|\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"SELECT add_one(a) FROM table_groupby\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".obenkyo_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
